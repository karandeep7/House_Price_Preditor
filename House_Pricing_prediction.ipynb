{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6++7Ltu7AEBdKapkba9bc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karandeep7/House_Price_Preditor/blob/main/House_Pricing_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XFGCMW5dmzJD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import zipfile\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import KNNImputer, IterativeImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"date\", exist_ok=True)\n",
        "\n",
        "# Download the dataset using Kaggle API\n",
        "dataset_name = \"ahmedshahriarsakib/usa-real-estate-dataset\"\n",
        "output_path = \"../data/\"\n",
        "\n",
        "# Construct the Kaggle command\n",
        "download_command = f\"kaggle datasets download -d {dataset_name} -p {output_path}\"\n",
        "\n",
        "# Execute the command\n",
        "os.system(download_command)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf-ziPkenJQj",
        "outputId": "e404c6b4-15d7-4eea-ea78-62ab594f56dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = os.path.join(output_path, f\"{dataset_name.split('/')[-1]}.zip\")\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(output_path)\n",
        "    os.remove(zip_path)  # Remove the ZIP file after extraction\n",
        "\n",
        "print(\"Dataset downloaded and unzipped to 'data/' folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ-R4qr7nUAU",
        "outputId": "68fab3dc-8d62-486c-eef9-cc535ba32080"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and unzipped to 'data/' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "file_path = \"../data/realtor-data.zip.csv\"  # Update with the actual file name\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check the data\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8LcSRwpnmxk",
        "outputId": "322bf7cd-21cd-4523-f548-b4c2098ab25d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (2226382, 12)\n",
            "   brokered_by    status     price  bed  bath  acre_lot     street  \\\n",
            "0     103378.0  for_sale  105000.0  3.0   2.0      0.12  1962661.0   \n",
            "1      52707.0  for_sale   80000.0  4.0   2.0      0.08  1902874.0   \n",
            "2     103379.0  for_sale   67000.0  2.0   1.0      0.15  1404990.0   \n",
            "3      31239.0  for_sale  145000.0  4.0   2.0      0.10  1947675.0   \n",
            "4      34632.0  for_sale   65000.0  6.0   2.0      0.05   331151.0   \n",
            "\n",
            "         city        state  zip_code  house_size prev_sold_date  \n",
            "0    Adjuntas  Puerto Rico     601.0       920.0            NaN  \n",
            "1    Adjuntas  Puerto Rico     601.0      1527.0            NaN  \n",
            "2  Juana Diaz  Puerto Rico     795.0       748.0            NaN  \n",
            "3       Ponce  Puerto Rico     731.0      1800.0            NaN  \n",
            "4    Mayaguez  Puerto Rico     680.0         NaN            NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#missing values\n",
        "print(\"Missing Values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pp5qLx1nuFI",
        "outputId": "c6a05fbe-5c4b-4904-c7e1-ef9fcce5f3bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values:\n",
            " brokered_by         4533\n",
            "status                 0\n",
            "price               1541\n",
            "bed               481317\n",
            "bath              511771\n",
            "acre_lot          325589\n",
            "street             10866\n",
            "city                1407\n",
            "state                  8\n",
            "zip_code             299\n",
            "house_size        568484\n",
            "prev_sold_date    734297\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Drop Non-Critical Columns\n",
        "columns_to_drop = ['brokered_by', 'street', 'prev_sold_date']\n",
        "df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "# Check the updated dataset\n",
        "print(\"Updated Dataset Shape:\", df.shape)\n",
        "print(\"Remaining Columns:\", df.columns)"
      ],
      "metadata": {
        "id": "b_QcPSlcn3Co",
        "outputId": "fc886702-076c-44f0-c2c1-0ef650bb45ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Dataset Shape: (2226382, 9)\n",
            "Remaining Columns: Index(['status', 'price', 'bed', 'bath', 'acre_lot', 'city', 'state',\n",
            "       'zip_code', 'house_size'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Identify important and non-important fields\n",
        "important_numeric_cols = ['bed', 'bath', 'house_size']  # Most important numeric fields\n",
        "other_numeric_cols = ['acre_lot', 'street']  # Less important numeric fields\n",
        "\n",
        "important_categorical_cols = ['status', 'city', 'zip_code']  # Most important categorical fields\n",
        "non_important_categorical_col = ['state']  # Less important categorical fields\n",
        "\n",
        "# 2. Encode categorical columns\n",
        "label_encoders = {}\n",
        "print(\"Encoding categorical columns...\")\n",
        "for col in tqdm(important_categorical_cols + non_important_categorical_col, desc=\"Encoding Categorical Columns\"):\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le  # Save encoders for later decoding\n",
        "\n",
        "# 3. KNN Imputation for the most important numeric fields (Parallelized)\n",
        "def knn_impute_chunk(chunk, cols, n_neighbors=5):\n",
        "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
        "    return imputer.fit_transform(chunk[cols])\n",
        "\n",
        "# Split data into chunks for parallel processing\n",
        "chunk_size = 500000\n",
        "chunks = [df.iloc[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
        "\n",
        "print(\"\\nImputing missing values for important numeric fields (KNN)...\")\n",
        "knn_imputed_chunks = Parallel(n_jobs=-1)(\n",
        "    delayed(knn_impute_chunk)(chunk, important_numeric_cols) for chunk in chunks\n",
        ")\n",
        "\n",
        "# Merge the results back into the DataFrame\n",
        "for i, chunk in enumerate(knn_imputed_chunks):\n",
        "    df.iloc[i * chunk_size:(i + 1) * chunk_size, df.columns.get_indexer(important_numeric_cols)] = chunk\n",
        "\n",
        "# 4. Iterative Imputation for the other numeric fields\n",
        "print(\"\\nImputing missing values for other numeric fields (Iterative)...\")\n",
        "iterative_imputer = IterativeImputer(max_iter=10, random_state=0)\n",
        "df[other_numeric_cols] = iterative_imputer.fit_transform(df[other_numeric_cols])\n",
        "\n",
        "# 5. KNN Imputation for important categorical fields\n",
        "print(\"\\nImputing missing values for important categorical fields (KNN)...\")\n",
        "categorical_knn_imputed_chunks = Parallel(n_jobs=-1)(\n",
        "    delayed(knn_impute_chunk)(chunk, important_categorical_cols) for chunk in chunks\n",
        ")\n",
        "\n",
        "# Merge the results back into the DataFrame\n",
        "for i, chunk in enumerate(categorical_knn_imputed_chunks):\n",
        "    df.iloc[i * chunk_size:(i + 1) * chunk_size, df.columns.get_indexer(important_categorical_cols)] = chunk\n",
        "\n",
        "# 6. Simple Imputation for non-important categorical column (`state`)\n",
        "print(\"\\nImputing missing values for non-important categorical column (Mode)...\")\n",
        "df['state'] = df['state'].fillna(df['state'].mode()[0])  # Replace NaNs with the most common value\n",
        "\n",
        "# Final imputed dataframe is df\n",
        "print(\"\\nImputation complete. Dataset after imputation:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "KjAEGHutmrTd",
        "outputId": "278ad3b2-a61d-42f3-e2aa-aaefd1fa2559",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding categorical columns...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding Categorical Columns: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Imputing missing values for important numeric fields (KNN)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}